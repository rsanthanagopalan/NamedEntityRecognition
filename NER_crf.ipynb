{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsanthanagopalan/NamedEntityRecognition/blob/main/NER_crf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install sklearn_crfsuite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc2B0UvCw9Td",
        "outputId": "735599b5-e0f7-47e5-d0b1-9e1c25426f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn==0.22.2 (from versions: 0.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for sklearn==0.22.2\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.9.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhDSqSB5w2bl",
        "outputId": "74d5ed93-e063-4236-b4f0-51ede2951e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.metrics import make_scorer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import scipy\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "  \n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkk3JFzyw2bo"
      },
      "outputs": [],
      "source": [
        "\n",
        "file=r\"/content/drive/MyDrive/Upgrad_sessions/NER/Example1/ner.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx7RM15xw2bp"
      },
      "outputs": [],
      "source": [
        "#reading data\n",
        "with open(file, 'r', encoding = 'ISO-8859-1') as f:\n",
        "    data=f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOSgGuhXw2bp"
      },
      "outputs": [],
      "source": [
        "#creating list of list of tuples\n",
        "docs=[]\n",
        "doc=[]\n",
        "for sent in data:\n",
        "    if(len(sent)==1):\n",
        "        docs.append(doc)\n",
        "        doc=[]\n",
        "    else:\n",
        "        word1,word2=sent.split()\n",
        "        word_tuple=(word1,word2[-2:])\n",
        "        doc.append(word_tuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vgaQN_9w2bq",
        "outputId": "be2e7fb0-e81b-481e-829f-7e0f3b64a2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('All', 'O'), ('live', 'O'), ('births', 'O'), ('>', 'O'), ('or', 'O'), ('=', 'O'), ('23', 'O'), ('weeks', 'O'), ('at', 'O'), ('the', 'O'), ('University', 'O'), ('of', 'O'), ('Vermont', 'O'), ('in', 'O'), ('1995', 'O'), ('(', 'O'), ('n', 'O'), ('=', 'O'), ('2395', 'O'), (')', 'O'), ('were', 'O'), ('retrospectively', 'O'), ('analyzed', 'O'), ('for', 'O'), ('delivery', 'O'), ('route', 'O'), (',', 'O'), ('indication', 'O'), ('for', 'O'), ('cesarean', 'O'), (',', 'O'), ('gestational', 'O'), ('age', 'O'), (',', 'O'), ('parity', 'O'), (',', 'O'), ('and', 'O'), ('practice', 'O'), ('group', 'O'), ('(', 'O'), ('to', 'O'), ('reflect', 'O'), ('risk', 'O'), ('status', 'O'), (')', 'O')]\n"
          ]
        }
      ],
      "source": [
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo-H2mZvw2br",
        "outputId": "c6ad922a-6551-416d-a376-d1f2732bec7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('All', 'DT', 'O'), ('live', 'JJ', 'O'), ('births', 'NNS', 'O'), ('>', 'VBP', 'O'), ('or', 'CC', 'O'), ('=', 'VBP', 'O'), ('23', 'CD', 'O'), ('weeks', 'NNS', 'O'), ('at', 'IN', 'O'), ('the', 'DT', 'O'), ('University', 'NNP', 'O'), ('of', 'IN', 'O'), ('Vermont', 'NNP', 'O'), ('in', 'IN', 'O'), ('1995', 'CD', 'O'), ('(', '(', 'O'), ('n', 'IN', 'O'), ('=', 'NNP', 'O'), ('2395', 'CD', 'O'), (')', ')', 'O'), ('were', 'VBD', 'O'), ('retrospectively', 'RB', 'O'), ('analyzed', 'VBN', 'O'), ('for', 'IN', 'O'), ('delivery', 'NN', 'O'), ('route', 'NN', 'O'), (',', ',', 'O'), ('indication', 'NN', 'O'), ('for', 'IN', 'O'), ('cesarean', 'NN', 'O'), (',', ',', 'O'), ('gestational', 'JJ', 'O'), ('age', 'NN', 'O'), (',', ',', 'O'), ('parity', 'NN', 'O'), (',', ',', 'O'), ('and', 'CC', 'O'), ('practice', 'NN', 'O'), ('group', 'NN', 'O'), ('(', '(', 'O'), ('to', 'TO', 'O'), ('reflect', 'VB', 'O'), ('risk', 'NN', 'O'), ('status', 'NN', 'O'), (')', ')', 'O')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Appending the POS tags\n",
        "data=[]\n",
        "for doc in docs:\n",
        "    words = [word for word,label in doc]\n",
        "    pos_tags=nltk.pos_tag(words)\n",
        "    data_sent=[]\n",
        "    for i in range(len(pos_tags)):\n",
        "        data_sent.append((doc[i][0],pos_tags[i][1],doc[i][1]))\n",
        "    data.append(data_sent)\n",
        "    \n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1fU_RBxw2br"
      },
      "outputs": [],
      "source": [
        "# features from word net \n",
        "#WordNet is a lexical database for the English language.[1] It groups English words into sets of synonyms called synsets, \n",
        "#provides short deÔ¨Ånitions and usage examples, and records a number of relations among\n",
        "#these synonym sets or their members. So for every token, the no of synsets corresponding to the\n",
        "#token are calculated which provides the number of contexts in which the corresponding token can be used\n",
        "\n",
        "def no_of_contexts(word):\n",
        "    temp=0\n",
        "    for syn in wn.synsets(word):\n",
        "        temp+=1\n",
        "    return temp\n",
        "\n",
        "# if it is alphanumeric\n",
        "def contain_digit(word):\n",
        "    for ch in list(word):\n",
        "        if ch.isdigit()==True:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vioob5how2bs"
      },
      "outputs": [],
      "source": [
        "#print the report\n",
        "\n",
        "def showreport(y_test,y_pred):\n",
        "    label_dict = {\"O\": 0, \"D\": 1,\"T\":2}\n",
        "   # creating predicted list of entities\n",
        "    model_output=[]\n",
        "    for row in y_pred:\n",
        "        for entity in row:\n",
        "            model_output.append(label_dict[entity])\n",
        "    #creating true list of entities\n",
        "    true_output=[]\n",
        "    for row in y_test:\n",
        "        for entity in row:\n",
        "            true_output.append(label_dict[entity])       \n",
        "    \n",
        "    # Print out the classification report\n",
        "    print(classification_report(true_output, model_output, target_names=[\"O\", \"D\",\"T\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqpf1kYKw2bt"
      },
      "outputs": [],
      "source": [
        "def word_to_features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        #hand-crafted conditions\n",
        "        'suffix_3': word[-3:],\n",
        "        #'suffix_2': word[-2:],\n",
        "        'prefix_3':word[:3],\n",
        "        'wordlen':len(word),\n",
        "       'word.isupper': word.isupper(),\n",
        "     'word.isdigit': contain_digit(word),\n",
        "      'postag': postag,\n",
        "        'no_of_contexts':no_of_contexts(word),\n",
        "        'word_stem':sno.stem(word.lower())\n",
        "\n",
        "      }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "\n",
        "         \n",
        "              '-1:wordlen': len(word1),\n",
        "           '-1:word.isupper': word1.isupper(),\n",
        "         '-1:word.isdigit': contain_digit(word1),\n",
        "         '-1:postag': postag1,\n",
        "            '-1:no_of_contexts':no_of_contexts(word1)\n",
        "            \n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "\n",
        "          '+1:word.isupper': word1.isupper(),\n",
        "               '+1:wordlen': len(word1),\n",
        "           '+1:word.isdigit':contain_digit(word1),\n",
        "           '+1:postag': postag1,\n",
        "           '+1:no_of_contexts':no_of_contexts(word1)\n",
        "            \n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZpjzVCyw2bt"
      },
      "outputs": [],
      "source": [
        "# convert words of each document into features reresented in form of dictionary\n",
        "X=[]\n",
        "Y=[]\n",
        "for doc in data:\n",
        "    X.append([word_to_features(doc,i) for i in range(len(doc))])\n",
        "    final_y=[label for (word,pos_tag,label) in doc]\n",
        "    Y.append(final_y)\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA1o2g-_w2bu"
      },
      "outputs": [],
      "source": [
        "#splitting in ratio of 70:10:20\n",
        "X_train, X_testanddev, y_train, y_testanddev= train_test_split(X, Y, test_size=0.3,random_state=4)\n",
        "\n",
        "X_test,X_dev, y_test,y_dev = train_test_split(X_testanddev, y_testanddev, test_size=0.33,random_state=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Bm77J2_Uw2bu"
      },
      "outputs": [],
      "source": [
        "# hyper parameter tunning \n",
        "#code referred from crf suite examples\n",
        "\n",
        "#https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#sklearn_crfsuite.CRF\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
        "labels=[\"D\",\"T\",\"O\"]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', #gradient descent\n",
        "                           max_iterations=1000, \n",
        "                           all_possible_transitions=True,\n",
        "                           verbose=False)\n",
        "\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
        "                        average='macro', labels=labels)\n",
        "\n",
        "rs = RandomizedSearchCV(crf, params_space,\n",
        "                        cv=10, #10-fold CV\n",
        "                        verbose=1,\n",
        "                        n_jobs=1, # JObs to run in parallel\n",
        "                        n_iter=20, #Number of parameter settings that are sampled\n",
        "                        scoring=f1_scorer)\n",
        "try:\n",
        "    rs.fit(X_train, y_train)\n",
        "except AttributeError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObCNsswGw2bv",
        "outputId": "06baa840-2f08-4b56-aee1-70c72432fd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for D, T and O label(average) is 0.785175 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.95      0.97      0.96     11355\n",
            "           D       0.75      0.71      0.73       899\n",
            "           T       0.76      0.59      0.66       788\n",
            "\n",
            "    accuracy                           0.93     13042\n",
            "   macro avg       0.82      0.76      0.79     13042\n",
            "weighted avg       0.93      0.93      0.93     13042\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# fitting the models with obtained hyperparameters c1=.055 and c2=.066\n",
        "\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=0.055,c2=0.066 ,\n",
        "                           max_iterations=1000,\n",
        "                           all_possible_transitions=True,\n",
        "                           verbose=False)\n",
        "crf.fit(X_train,y_train)\n",
        "labels=[\"O\",\"D\",\"T\"]\n",
        "\n",
        "#predicting the entities for test data\n",
        "y_pred=crf.predict(X_test)\n",
        "print(\"F1 score for D, T and O label(average) is %lf \"% (metrics.flat_f1_score(y_test, y_pred,\n",
        "                      average='macro', labels=labels)))\n",
        "#printing the classfication report\n",
        "showreport(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mvcUoIOw2bv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}